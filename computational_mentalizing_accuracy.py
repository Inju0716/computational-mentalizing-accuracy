# -*- coding: utf-8 -*-
"""computational_mentalizing_accuracy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RaTiVdohvnzm-WIWv_SDvsKW0Ku-ElmY
"""

import pandas as pd
from sentence_transformers import util

# install the sentence-transformers library (used for generating SBERT embeddings)
!pip install -U sentence-transformers

# load a pretrained KR-SBERT (Korean SBERT) model
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')

# convert a text into a vector embedding using KR-BERT model
def vectorize_text(text):
  return model.encode(text)

# compute cosine similarity between two SBERT embeddings
# this value is used as computational mentalizing accuracy
def compute_cos_sim(embedding1, embedding2):
  return util.pytorch_cos_sim(embedding1, embedding2)